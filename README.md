# MScAC-Research-Project
The prevalence of harmful content in online spaces necessitates robust and efficient content moderation systems. In this study, we evaluate three prominent large language models (LLMs)—Fine-tuned RoBERTa, the Moderation API, and GPT-4—across a diverse dataset containing aspects of harmful content, including harassment, hate speech, sexual content, self-harm, and violence. Our aim is to assess their performance in automating harmful content detection and provide insights for real-world applications. Our results demonstrate the superior performance of RoBERTa, which consistently achieves the highest accuracy and F1 score. It showcases the potential of fine-tuned pre-trained language models in content moderation. GPT-4 exhibits content moderation capabilities with the flexibility to tailor harmful content guidelines, offering a faster feedback loop for policy refinement. The Moderation API excels in precision and recall, highlighting its suitability for specific content moderation tasks. We also present a nuanced analysis of the pros and cons of applying these LLMs in real-world scenarios, considering factors such as task type, runtime requirements, and cost.

You can find the complete report and the code part in LLMs.ipynb file.
